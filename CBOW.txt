import numpy as np
import re
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Lambda
import matplotlib.pyplot as plt

data = """The Continuous Bag of Words is a natural language processing technique to generate word embeddings. 
Word embeddings are useful for many NLP tasks as they represent semantics and structural connections amongst words in a language."""

EMBEDDING_DIM = 10
CONTEXT_SIZE = 2 
INPUT_LENGTH = CONTEXT_SIZE * 2

def preprocess_text(data):
    sentences = data.split('.')
    clean_sent = []
    for sentence in sentences:
        if sentence.strip():
            sentence = re.sub('[^A-Za-z0-9]+', ' ', sentence)
            sentence = sentence.strip().lower()
            clean_sent.append(sentence)
    return clean_sent

clean_sentences = preprocess_text(data)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(clean_sentences)

word_to_index = tokenizer.word_index
index_to_word = tokenizer.index_word
vocab_size = len(word_to_index) + 1 # +1 for 0-padding

sequences = tokenizer.texts_to_sequences(clean_sentences)

contexts = []
targets = []

for sequence in sequences:
    if len(sequence) < INPUT_LENGTH + 1:
        continue
        
    for i in range(CONTEXT_SIZE, len(sequence) - CONTEXT_SIZE):
        targets.append(sequence[i])
        
        context = (
            sequence[i - CONTEXT_SIZE : i] + 
            sequence[i + 1 : i + CONTEXT_SIZE + 1]
        )
        contexts.append(context)

X = np.array(contexts)
Y = np.array(targets)

model = Sequential([
    Embedding(input_dim=vocab_size, 
              output_dim=EMBEDDING_DIM),
    Lambda(lambda x: tf.reduce_mean(x, axis=1)),
    Dense(128, activation='relu'),
    Dense(vocab_size, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])

history = model.fit(X, Y, epochs=150, verbose=0)

plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['accuracy'], label='Accuracy')
plt.title('Training History')
plt.xlabel('Epoch')
plt.legend()
plt.grid(True)
plt.show()

test_sentences = [
    "continuous bag words is",      
    "natural language technique to",  
    "technique to word embeddings", 
]

print("\n--- Predictions ---")
for sent in test_sentences:
    test_words = sent.split(" ")
    
    x_test = [word_to_index.get(word) for word in test_words]
    x_test = np.array([x_test])
    
    pred_probs = model.predict(x_test, verbose=0)
    pred_index = np.argmax(pred_probs[0])
    pred_word = index_to_word.get(pred_index)
    
    print(f"Context: '{sent}' \nPredicted: '{pred_word}'\n")